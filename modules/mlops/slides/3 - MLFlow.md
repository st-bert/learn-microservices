# 1. Introduction

MLflow is an open-source platform designed for managing the machine learning experiment lifecycle, aimed at simplifying and optimizing the development, monitoring, and deployment processes of models. With the goal of making experiment management more systematic and organized, MLflow enables users to log, monitor, and analyze every aspect of their machine learning activities, thereby promoting a data-driven approach.

In the context of machine learning, effectively managing various experiments, the parameters used, and the results obtained is essential. The challenges associated with this task can include:

- **Reproducibility**: Ensuring that experiments can be replicated in the future with consistent results.
- **Comparison**: Evaluating the effectiveness of different model configurations and parameters.
- **Documentation**: Maintaining a detailed record of decisions and outcomes for future reference.

MLflow addresses these challenges by offering a comprehensive suite of tools and features that simplify the lives of data science teams. Its intuitive interface and logging capabilities facilitate collaboration among team members and make it easier to share and review results.

MLflow seamlessly integrates into the **MLOps** (Machine Learning Operations) paradigm, an approach that combines development and operational practices to ensure efficiency and quality in machine learning projects. Through MLOps, MLflow allows for the management of the entire model lifecycle, from design and development to deployment and post-launch monitoring. This approach not only enhances consistency and transparency in the development process but also fosters a culture of collaboration and continuous improvement among data science and IT teams.

![](images/mlflow-logo.webp)

---

# 2. MLflow in Tracking Machine Learning Experiments

The main feature of MLflow is **experiment tracking**. This functionality allows users to log and track parameters, metrics, and artifacts of each experiment. This way, users can effectively monitor model performance and compare different configurations, helping to maintain transparency in the development process and facilitating the sharing of results with the team.

## 1. Metadata and Artifacts in MLflow

MLflow distinguishes between two main types of information: **metadata** and **artifacts**. This distinction is essential for understanding how experiment data is managed and stored.

### Metadata
Metadata includes all structured information related to experiments and runs. This data is typically stored in the **database** configured for MLflow and includes:

- **Parameters**: Settings used to configure the models, such as training hyperparameters.
- **Metrics**: Performance measures calculated during or after training, such as accuracy, F1 score, or execution time.
- **Tags**: Labels that can be assigned to experiments and runs to add descriptive information (e.g., the type of experiment or the owner).
- **Run information**: Specific details for each execution, such as the experiment ID, timestamp, configuration, and final status.

This data is saved in a **relational database** (like MySQL, PostgreSQL, or SQLite), facilitating the search, filtering, and comparison of runs and their results.

### Artifacts
Artifacts, on the other hand, are the files generated during the runs and include larger, unstructured items such as:

- **Trained models**: Files containing the saved models.
- **Graphs**: Visual representations of performance, such as accuracy or loss curves.
- **Configuration files**: Scripts or other files necessary to reproduce the experiment.

Artifacts **are not stored in the database** since they take up more space and require different management. Instead, they are stored in a separate storage space, which can be a local folder or a cloud storage service.

![](images/mlflow-metadata.webp)

This separation between metadata and artifacts optimizes the efficiency of MLflow and allows for a more scalable management of experiment data.

---

## 2. Setup

### 1. Automatic Database Creation

Before starting experiments and runs, MLflow requires a database where all metadata related to the experiments will be stored. This database can be configured to be automatically generated by MLflow, ensuring that all necessary tables for tracking executions and associated information are created. 

To enable automatic database creation, simply configure MLflow to use an external database, such as MySQL.

```python
mlflow.set_tracking_uri("mysql+pymysql://username:password@host:port/db_name")
``` 

In the code above:

- `mysql` indicates the type of database to connect to, in this case, MySQL. MLflow uses SQLAlchemy, a Python toolkit for managing database connections, and `mysql` is specified as the type of database to connect to.

- `pymysql` is the specific driver for connecting Python to MySQL. It is a library that handles communication between the Python language and the MySQL database, allowing Python code to send and receive data from the database.

Once configured, MLflow will check for the presence of the database and the following tables upon the first request. If they do not exist, they will be created automatically in the specified database. Below is an overview of the main tables generated.

1. **alembic_version**  
   This table is used internally by MLflow to track the version of the database schema. It helps maintain compatibility for potential database updates with new versions of MLflow.

2. **dataset**  
   Contains information about the datasets used in experiments, helping to maintain the link between models and the data they were trained on.

3. **experiment_tags**  
   This table stores tags associated with experiments, allowing for annotation of each experiment with useful details like owner, description, or other custom information.

4. **experiments**  
   Stores basic information about each experiment, such as name, ID, and current status (e.g., active or archived).

5. **input_tags**  
   Contains specific tags for the inputs of each run or experiment, allowing for more detailed organization of input data.

6. **input**  
   Gathers information about the inputs used in experiments, recording details like the type and format of the initial data.

7. **latest_metrics**  
   Stores the most recent metrics calculated for each run of an experiment, facilitating access to the most up-to-date results without having to search through all historical records.

8. **metrics**  
   Preserves all metrics logged during the runs of experiments, including data such as precision, accuracy, execution time, and other performance measures of the runs.

9. **model_version_tags**  
   Allows for associating tags with registered model versions, facilitating the organization and identification of various model versions.

10. **model_versions**  
    Stores details of each registered model version, including references to the model, version ID, and any deployment information.

11. **params**  
    Logs all parameters associated with each run of an experiment, such as specific hyperparameters and configured values for each model execution.

12. **registered_model_aliases**  
    Contains aliases for registered models, making it easier to manage models with alternative or more descriptive names.

13. **registered_model_tags**  
    Allows tags to be assigned to registered models, adding useful information about each model, such as version, model type, or other notes.

14. **registered_models**  
    Manages basic information for registered models, such as name, status, and model description.

15. **runs**  
    This table logs each execution (run) of an experiment, retaining details about the configuration, parameters, timestamps, and outcomes of the runs.

16. **tags**  
    Enables the assignment of tags to both experiments and individual runs, facilitating the categorization and search of executions or experiments based on specific tags.

17. **trace_info**  
    Contains tracking information to better monitor the origin of the data or events related to each experiment.

18. **trace_request_metadata**  
    This table collects metadata for specific requests related to the experiments, such as the source of data requests or the type of event.

19. **trace_tags**  
    Gathers specific tags for tracking information, allowing for the categorization and filtering of traces for more detailed analysis.

---

### 2. Creation of experiments

In MLflow, an **experiment** represents a working context for a set of related **runs**. Each experiment has a unique ID that provides access to all associated runs and related information.

To create an experiment, the following function is used:

```python
experiment_id = mlflow.create_experiment(
    experiment_name, 
    tag={"description": description, "owner": owner}
) 
```

The parameters of the function are:

- **name**: the name of the experiment, which must be a unique string.
- **artifact_location**: the location where run artifacts are stored. If not provided, the server will choose an appropriate default value.
- **tags**: an optional dictionary of keys and string values to be set as tags on the experiment.

In the example, the experiment is created with a specific name and enriched with useful tags, such as the description and owner of the experiment. These tags are crucial to facilitate the search and organization of experiments within the platform.

After the experiment is created, it is assigned a unique ID, which is critical to retain for later access to runs and associated information.

### 3. Creation of Runs

In MLflow, a **run** represents a single execution of an activity related to a machine learning model within an experiment. 
This activity may include:
- the training of a model,
- the evaluation of its performance,
- the tuning of hyperparameters
- the generation of predictions. 

Each run is associated with a specific experiment and allows all details related to that activity to be recorded.

![](images/mlflow-exp-run.webp)

To create a run, you use the `mlflow.start_run()` function; An example of how to start a run is shown below:

```python
with mlflow.start_run(experiment_id=experiment_id, run_name="Nome della Run) as run:
```

The parameters that can be specified in the function are:

- **experiment_ids** or **experiment_names** : List of experiment IDs from which to search for runs. You can specify only IDs or names, but not both.
  
- **filter_string**: A string to filter the results of the runs. For example, you can search for runs with specific metrics.

- **run_view_type**: Specifies the view type of runs (active, deleted, or all).

- **max_results**: Maximum number of runs to return (default: 100,000).

- **order_by**: List of columns to order the results (e.g., “metrics.rmse”).

- **search_all_experiments**: Boolean to search all experiments.

- **output_format**: Specifies output format (can be `pandas` or `list`).

A list of `mlflow.entities.Run` objects or, if specified, a `pandas.DataFrame` with run information will be returned.

During a run, you can also record contextual information using custom tags. For example, you can set tags that describe the type of experiment or model used, as in the following example:

```python
mlflow.set_tag("experiment_type", "GridSearch_CrossValidation")
mlflow.set_tag("model_type", model_name)
```
---

## 3. Logging of Runs Information

After starting a run, you can log several pieces of information that are critical for analyzing and comparing model performance. In MLflow, you can log various types of data during a run, including:

### 1. Parameters

You can record not only the hyperparameters used to train the model, but also other aspects such as parameters of the dataset and process configurations.

```python
# Model parameters log
mlflow.log_params({f"model_{k}": v for k, v in best_params.items()})

# Dataset feature log
mlflow.log_param("data_samples", X_train.shape[0])
mlflow.log_param("data_features", X_train.shape[1])
mlflow.log_param("data_classes", len(set(y_train)))

# Parameter log for grid search
mlflow.log_params({f"grid_search_{k}": v for k, v in param_grid[model_name].items()})
mlflow.log_param("grid_search_scoring", scoring_grid_search)
mlflow.log_param("grid_search_cv_folds", cv)
```

### 2. Metrics

Model performance results can be recorded here for comparison across runs, but other metrics can also be considered that can provide valuable information about the context in which the process was run. 

```python
# Model performance metrics log
mlflow.log_metrics({f “model_{k}”: v for k, v in metrics.items()})

# Log of system metrics.
mlflow.log_metric(“system_cpu_usage”, result['cpu_usage'])
mlflow.log_metric(“system_execution_time”, result['execution_time'])
```

### 3. Artifacts

Files and objects generated during the training process can be logged here. In the following code, for example, it is shown how to log a trained model as an artifact:

```python
mlflow.sklearn.log_model(
sk_model=best_model,
artifact_path=“model”,
signature=signature,
input_example=input_example
)
```

In this case, model artifacts include:
- **Serialized model**: the trained model, saved as a serialized file, which allows it to be restored and used later for predictions.
- **Model Dependencies**: files such as `conda.yaml` or `requirements.txt` that describe the execution environment, useful for replicating the software environment in which the model was trained.
- **Signature and Input Example**: `signature` contains the input and output format of the model, while `input_example` is an input example that helps identify the data type expected by the model.

    The artifacts are saved in a relative directory specified by the `artifact_path` parameter, which is registered under the current run. To retrieve the model later, the full path to the artifact is specified as `runs:/<run_id>/model`.

In addition to the artifacts, the **metadata** of the model is also saved in the MLflow database, which includes:

- **Run ID**: a unique identifier for the current run that associates the model with the experiment to which it belongs.
- **Artifact path**: a reference to the storage path of the model (such as `runs:/<run_id>/model`), which is needed to locate and retrieve the saved model.
- **Parameters and metrics**: If the model is associated with a run with logged parameters and metrics, this data is included in the database, allowing configurations and training performance to be tracked.
- **Time and logging information**: timestamps and other details that track log configuration, useful for model context and versioning.

This metadata, although not containing the model itself, provides all the information needed to identify and load the correct model from the artifact directory.

---

## 4. Information Retrieval

MLflow offers several APIs to access information about recorded experiments and runs. These APIs allow retrieval of parameters, metrics, tags, and context for each run, facilitating analysis and comparison between runs. Some of the main features for searching and retrieving experiments and runs are described below.

### 1. API for retrieving experiments and runs

```
mlflow.search_experiments(view_type: int = 1, max_results: Optional[int] = None, filter_string: Optional[str] = None, order_by: Optional[List[str]] = None) → List[Experiment]
```
This function returns a list of experiments that meet the specified search criteria. Each experiment represents a group of related runs and is identified by a unique ID.

```python
experiments = mlflow.search_experiments()
```
**Main parameters:**

- **`view_type`**: Specifies which experiments to display, with options such as `ACTIVE_ONLY` (active experiments only), `DELETED_ONLY` (deleted experiments) and `ALL`.
- **`max_results`**: Limits the number of results. If not set, returns all experiments found.
- **`filter_string`**: Allows filtering by attributes such as `name`, `creation_time` or `tags.<tag_key>`. 
For example, `filter_string=“name = 'experiment_name`”` returns only experiments with that name.
- **`order_by`**: Defines the display order. Examples of sorting are `“name DESC”` by decreasing name and `“last_update_time ASC”` by increasing update date.

The output is a list of `Experiment` objects, containing information such as name, ID and creation timestamp.

---

```
mlflow.search_runs(experiment_ids: Optional[List[str]] = None, filter_string: str = '', run_view_type: int = 1, max_results: int = 100000, order_by: Optional[List[str]] = None, output_format: str = 'pandas', search_all_experiments: bool = False, experiment_names: Optional[List[str]] = None) → Union[List[Run], pandas.DataFrame][source]**
```
This function searches for all runs belonging to one or more experiments. Each run is a single execution of the training process.

```python
runs = mlflow.search_runs(experiment_ids=[exp.experiment_id])
```

**Main parameters:**

- **`experiment_ids`**: A list of experiment IDs in which to search for runs. Alternatively, you can specify `experiment_names`.
- **`filter_string`**: Filters the runs based on specific parameters. For example, `“metrics.rmse < 0.2”` selects runs with a value of `rmse` less than 0.2.
- **`run_view_type`**: Indicates the type of run to search for, with options such as `ACTIVE_ONLY`, `DELETED_ONLY`, or `ALL`.
- **`max_results`**: Maximum number of runs to include in the result.
- **`order_by`**: Specifies the order of the results, such as `metrics.accuracy DESC` to sort by accuracy in descending order.
- **`output_format`**: Output format, can be `pandas` (returns a DataFrame) or `list` (returns a list of Run objects).

The output is a list of `Run` objects, each containing run-related details such as parameters, metrics, and tags.

---

```mlflow.get_experiment(experiment_id: str) → Experiment[source]```

This function allows retrieving a specific experiment using its ID.

**Parameters:**

- **`experiment_id`**: The unique ID of the experiment, returned by the `create_experiment` function.

**Output:** Returns an `mlflow.entities.Experiment` object that includes detailed information about the experiment, such as name, ID and location of artifacts.

---
```
mlflow.get_run(run_id: str) → Run
```
This function allows a specific run to be retrieved via its `run_id`, reporting a range of relevant information about logged parameters, tags and metrics, along with model inputs (dataset) and other metadata.

**Parameters:**

- **`run_id`**: The unique identifier of the run.

**Output:** Returns a `Run` object, which includes:

- **RunInfo**: Metadata about the run, such as the run ID, associated experiment and lifecycle state.
- **RunData**: Parameters, tags and metrics about the run. In case of multiple metrics with the same key, the last value logged at the highest step for each metric is kept.
- **RunInputs** (experimental): Includes information about the datasets used for the run.

If the run with the specified `run_id` does not exist, an exception is raised.

---

Functions using **search** (such as `mlflow.search_experiments()` and `mlflow.search_runs()`) return collections of experiments or runs based on specific search criteria. They offer the ability to filter and sort results based on various parameters.

The **get** functions (such as `mlflow.get_experiment()` and `mlflow.get_run()`) are used to retrieve specific information about a single experiment or run using their unique identifier. These functions provide complete details about that specific object, but do not allow filtering or searching across multiple objects.

### 2. API for retrieving information from Experiments and Runs.

Once we have obtained the **Experiments** and **Runs** through the previously described methods, we can leverage this data to extract multiple useful pieces of information. The following are examples of how to use the MLflow API to retrieve detailed information:

1. **Fetching experiments and counting associated runs**: In this example, we use `mlflow.search_experiments()` to get a list of experiments and then, for each experiment, we use `mlflow.search_runs()` to count how many runs are associated with each experiment. Finally, we store this information in a list.


 ```python
experiments = mlflow.search_experiments()
experiments_list = []

for exp in experiments:
   runs = mlflow.search_runs(experiment_ids=[exp.experiment_id])
   run_count = len(runs)

   experiments_list.append({
       “experiment_id": exp.experiment_id,
       “name": exp.name,
       “lifecycle_stage": exp.lifecycle_stage,
       “run_count": run_count
   })
```

2. **Analysis of runs of an experiment**: We can use `mlflow.search_runs()` to get a lot more information associated with Runs.

```python
experiment_runs = mlflow.search_runs([experiment_id])
report = {
    “number_of_runs": len(experiment_runs),
    “completed_runs“: sum(run[”status"] == ‘FINISHED’ for _, run in experiment_runs.iterrows()),
    “failed_runs“: sum(run[”status"] == ‘FAILED’ for _, run in experiment_runs.iterrows()),
    “active_runs“: sum(run[”status"] == ‘RUNNING’ for _, run in experiment_runs.iterrows()),
    “stopped_runs“: sum(run[”status"] == ‘STOPPED’ for _, run in experiment_runs.iterrows()),
    “first_run_completed“: min(run[”start_time"].strftime(‘%Y-%m-%d %H:%M:%S’) for _, run in experiment_runs.iterrows()),
    “last_run_completed“: max(run[”end_time"].strftime(‘%Y-%m-%d %H:%M:%S’) for _, run in experiment_runs.iterrows()),
}
```
In this example, a report is created by parsing all **Runs** of a specific **Experiment** by passing its id.


3. **Fetching information about an experiment**: We can use `mlflow.get_experiment` to access some information about the experiment as follows:

```python
experiment = mlflow.get_experiment(experiment_id)
experiment_info = {
    “experiment_id": experiment.experiment_id,
    “experiment_name": experiment.name,
    “artifact_location": experiment.artifact_location,
    “lifecycle_stage": experiment.lifecycle_stage,
    “tags": experiment.tags,
    “data_creation": datetime.fromtimestamp(experiment.creation_time / 1000).strftime(‘%Y-%m-%d %H:%M:%S’) if experiment.creation_time else None,
    “last_updated": datetime.fromtimestamp(experiment.last_update_time / 1000).strftime(‘%Y-%m-%d %H:%M:%S’) if experiment.last_update_time else None,
}
```


4. **Fetching the information of a run**: We can use `mlflow.get_run` to access the metrics, parameters, tags and information of a **Run**:

```python
run = self.mlflow.get_run(run_id)
run_metrics = run.data.metrics
params = run.data.params
run_name = run.data.tags.get('mlflow.runName', 'Unnamed Run')
model = run.data.tags.get('model_type', 'Unknown Model Type')
lifecycle_stage = run.info.lifecycle_stage
```

Having obtained this information, there are multiple uses that can be devised for filtering the data according to one's needs, creating custom functions or taking advantage of what MLflow offers.

---

# 3. Sources

- [Official MLflow Documentation](https://mlflow.org/docs/latest/index.html)
- [MLflow Tutorials](https://mlflow.org/docs/latest/getting-started/index.html)















